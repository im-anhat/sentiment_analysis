{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "255fc38d",
   "metadata": {
    "executionInfo": {
     "elapsed": 6159,
     "status": "ok",
     "timestamp": 1718307114518,
     "user": {
      "displayName": "Nhat Le",
      "userId": "15128282853851571850"
     },
     "user_tz": 300
    },
    "id": "255fc38d",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "from torch import nn\n",
    "import pandas as pd\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import json\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader,Dataset, TensorDataset\n",
    "import time\n",
    "from ast import literal_eval\n",
    "import torch.autograd as autograd\n",
    "from torch.utils.data import Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b0efef74",
   "metadata": {
    "executionInfo": {
     "elapsed": 188,
     "status": "ok",
     "timestamp": 1718307117722,
     "user": {
      "displayName": "Nhat Le",
      "userId": "15128282853851571850"
     },
     "user_tz": 300
    },
    "id": "b0efef74",
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "'''---Create data loader----'''\n",
    "\n",
    "class MovieDataset(Dataset):\n",
    "    def __init__(self, filename):\n",
    "        super().__init__()\n",
    "        self.df = pd.read_csv(filename, converters={'input_x': literal_eval})\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # load the input features\n",
    "        input_x = self.df['input_x'].iloc[index]\n",
    "        label = self.df['Label'].iloc[index]\n",
    "\n",
    "        return torch.tensor(input_x), torch.tensor(label,dtype=torch.float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d063ec7",
   "metadata": {
    "executionInfo": {
     "elapsed": 182,
     "status": "ok",
     "timestamp": 1718307126018,
     "user": {
      "displayName": "Nhat Le",
      "userId": "15128282853851571850"
     },
     "user_tz": 300
    },
    "id": "2d063ec7",
    "tags": []
   },
   "outputs": [],
   "source": [
    "## download the pretrain embedding\n",
    "def _read_glove_embedding(glove_file):\n",
    "    '''Fun:read embedding from pre-train by glove\n",
    "    '''\n",
    "    ## map word to embedding\n",
    "    word2embedding = dict()\n",
    "    with open(glove_file,\"r\") as f:\n",
    "        for num,line in enumerate(f):\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            emb = np.array(values[1:], dtype='float32')\n",
    "            word2embedding[word] = emb\n",
    "    # Return a dictionary mapping from each word to d-dimension vector from Glove\n",
    "    return word2embedding\n",
    "\n",
    "\n",
    "def _get_embedding(glove_file,tokens2index,embed_dim=200):\n",
    "    \"\"\"Fun:get the embedding matrix for our vocabulary\n",
    "    \"\"\"\n",
    "    ##load glove embedding to embedding matrix\n",
    "    word2embedding = _read_glove_embedding(glove_file)\n",
    "    embedding_matrix = np.zeros((len(tokens2index), embed_dim))\n",
    "    for word, i in tokens2index.items():\n",
    "        embedding_vector = word2embedding.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            # words not found in embedding index will be all-zeros.\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    # Return embedding matrix\n",
    "    return torch.FloatTensor(embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0aef73e6",
   "metadata": {
    "id": "0aef73e6",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    \"\"\"\n",
    "    LSTM model that will be used to perform Sentiment analysis.\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, output_size, embedding_dim, embedding_matrix,\\\n",
    "        hidden_dim, n_layers, input_len, pretrain=False):\n",
    "        \"\"\"\n",
    "        Initialize the model by setting up the layers.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.output_size = output_size  # y_out size = 1\n",
    "        self.n_layers = n_layers   # layers of LSTM\n",
    "        self.hidden_dim = hidden_dim  # hidden dim of LSTM\n",
    "        self.input_len = input_len # len of input features\n",
    "\n",
    "        ## set up pre-train embeddings. if true, load pretrain-embedding from GloVe\n",
    "        if pretrain:\n",
    "            # print(\"import glove embedding to nn.Embedding now\")\n",
    "            self.embedding = nn.Embedding.from_pretrained(embedding_matrix,freeze=False)\n",
    "        else:\n",
    "            self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "            self.init_weights()\n",
    "        ##-------------------------------------------\n",
    "        ## Q4: write the code to define LSTM model\n",
    "        ##-------------------------------------------\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, batch_first=True)\n",
    "        ## dropout layer\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "        ## max pool\n",
    "        self.pool = nn.MaxPool1d(self.input_len)\n",
    "\n",
    "        ## linear and sigmoid layers\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "\n",
    "    ## initialize the weights\n",
    "    def init_weights(self):\n",
    "        \"\"\"\n",
    "        Initializes some parameters with values from the uniform distribution, for easier convergence.\n",
    "        \"\"\"\n",
    "        self.embedding.weight.data.uniform_(-0.1, 0.1)\n",
    "\n",
    "    ## initial hidden state and cell state\n",
    "    def _init_hidden(self, batch_size, device):\n",
    "        return(autograd.Variable(torch.randn(self.n_layers, batch_size, self.hidden_dim)).to(device),\n",
    "                autograd.Variable(torch.randn(self.n_layers, batch_size, self.hidden_dim)).to(device)\n",
    "                )\n",
    "\n",
    "\n",
    "    ## feed input x into LSTM model for training/testing\n",
    "    def forward(self, x, device):\n",
    "        batch_size = x.size(0)\n",
    "        hidden_cell = self._init_hidden(batch_size, device)\n",
    "\n",
    "        ## feed words to get embeddings\n",
    "        embeds = self.embedding(x)\n",
    "\n",
    "        ## hidden_cell: (hidden, cell) where hidden=layer * batch * dim\n",
    "        lstm_out, _ = self.lstm(embeds, hidden_cell)\n",
    "        # print(\"size: \", lstm_out.size())  #2*100*50\n",
    "\n",
    "        ## permute the output dim\n",
    "        lstm_out = lstm_out.permute(0,2,1)\n",
    "\n",
    "        out = self.pool(lstm_out)\n",
    "        # print(\"pool size:\", out.size())\n",
    "\n",
    "        out = out.view(out.size(0),-1)\n",
    "\n",
    "        ## feed into linear layer\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc(out)\n",
    "        out = self.sigmoid(out)\n",
    "        out = out[:,0]\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "40e63e29",
   "metadata": {
    "id": "40e63e29",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def may_make_dir(path):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        path: a dir, or result of `os.path.dirname(os.path.abspath(file_path))`\n",
    "    Note:\n",
    "        `os.path.exists('')` returns `False`, while `os.path.exists('.')` returns `True`!\n",
    "    \"\"\"\n",
    "    # This clause has mistakes:\n",
    "    # if path is None or '':\n",
    "\n",
    "    if path in [None, '']:\n",
    "        return\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237e03c3-3ef2-4038-a272-2bffdbf36c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''save checkpoint'''\n",
    "def _save_checkpoint(ckp_file_path, model, epoches, global_step, optimizer):\n",
    "    checkpoint = {'epoch': epoches,\n",
    "                'global_step': global_step,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict()}\n",
    "    may_make_dir(os.path.dirname(os.path.abspath(ckp_file_path)))\n",
    "    torch.save(checkpoint, ckp_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "512c48d0",
   "metadata": {
    "id": "512c48d0",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "    print(\"device: \", device)\n",
    "\n",
    "    # Parameters\n",
    "    Batch_size = 64\n",
    "    n_layers = 2\n",
    "    input_len = 150\n",
    "    embedding_dim = 300 # As we are using glove embedding 300-D vectors\n",
    "    hidden_dim = 200\n",
    "    mode = 'train'\n",
    "    load_cpt = False #True\n",
    "    ckp_path = 'checkpoint'\n",
    "    resume_step = 0 # for resuming the model weight (only use when load_ckpt is True)\n",
    "    \n",
    "    # binary cross entropy\n",
    "    output_size = 1\n",
    "    num_epoches = 100\n",
    "    learning_rate = 0.001\n",
    "    clip = 5\n",
    "    embedding_matrix = None\n",
    "\n",
    "    pretrain = True # Load GloVe embedding matrix\n",
    "    if pretrain:\n",
    "        glove_file = 'data/glove.6B/glove.6B.300d.txt' # change this to the correct path of the Glove file that contain the vector we want to use\n",
    "        \n",
    "    training_set = MovieDataset('data/training_data.csv')\n",
    "    training_generator = DataLoader(training_set,batch_size=Batch_size,shuffle=True,num_workers=1)\n",
    "    test_set = MovieDataset('data/test_data.csv')\n",
    "    test_generator = DataLoader(test_set,batch_size=Batch_size,shuffle=False,num_workers=1)\n",
    "\n",
    "    # Read tokens & Load pretrained embedding matrix\n",
    "    with open('data/tokens2index.json', 'r') as f:\n",
    "        tokens2index = json.load(f)\n",
    "    vocab_size = len(tokens2index)\n",
    "\n",
    "    if pretrain:\n",
    "        embedding_matrix = _get_embedding(glove_file,tokens2index,embedding_dim)\n",
    "\n",
    "    # Model training\n",
    "    model = LSTMModel(vocab_size, output_size, embedding_dim, embedding_matrix,hidden_dim, n_layers, input_len,pretrain)\n",
    "    model.to(device)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(),lr=learning_rate)\n",
    "    criterion = nn.BCELoss()\n",
    "\n",
    "    # load checkpoint\n",
    "    if load_cpt:\n",
    "        print(\"*\"*10+'loading checkpoint'+'*'*10)\n",
    "        ckp_file_path = os.path.join(ckp_path, 'step_{}.pt'.format(resume_step))\n",
    "        # check if the ckpt exist\n",
    "        assert os.path.exists(ckp_file_path), 'The ckpt path is not exist!'\n",
    "        checkpoint = torch.load(ckp_file_path)\n",
    "        ## load parameters such as W and b to models\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        epoches = checkpoint['epoch']\n",
    "\n",
    "    ## model training\n",
    "    print('*'*89)\n",
    "    print('start model training now')\n",
    "    print('*'*89)\n",
    "    if mode == 'train':\n",
    "        model.train()\n",
    "        global_step = 0\n",
    "        loss_list = []\n",
    "        loss_per_epoch = []\n",
    "        loss_history = []\n",
    "        for epoches in range(num_epoches):\n",
    "            for x_batch, y_labels in training_generator:\n",
    "                global_step += 1\n",
    "                # print(\"step: \", global_step)\n",
    "                x_batch, y_labels = x_batch.to(device), y_labels.to(device)\n",
    "                y_out = model(x_batch, device)\n",
    "\n",
    "                ## loss fun\n",
    "                loss = criterion(y_out, y_labels)\n",
    "                \n",
    "                loss_list.append(loss.item())\n",
    "                loss_per_epoch.append(loss.item())\n",
    "\n",
    "                ## back propagation\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                ## clip_grad_norm helps prevent the exploding gradient problem in LSTMs.\n",
    "                nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "                optimizer.step()\n",
    "\n",
    "                if global_step % 50 == 0:\n",
    "                    ## y predict label\n",
    "                    y_pred_label = torch.round(y_out)\n",
    "                    print(\"step:{0} loss: {1}\".format(global_step,np.mean(loss_list)))\n",
    "                    loss_list = []\n",
    "        \n",
    "            loss_history.append(np.mean(loss_per_epoch)) \n",
    "            \n",
    "            # save checkpoint for each epoch\n",
    "            print(\"*** save checkpoint ****\")\n",
    "            ckpt_file_path = os.path.join(ckp_path, 'step_{}.pt'.format(global_step))\n",
    "            # ckp_path = 'checkpoint/step_{}.pt'.format(global_step)\n",
    "            _save_checkpoint(ckpt_file_path, model, epoches, global_step, optimizer)   \n",
    "\n",
    "    # Plotting the loss\n",
    "    plt.figure()\n",
    "    plt.plot(loss_history, label='Training Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training Loss over Epochs')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    # Plot loss after training\n",
    "    \n",
    "    ## model testing\n",
    "    print(\"----model testing now----\")\n",
    "    accy_count_total = 0\n",
    "    total_count = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for x_batch, y_labels in test_generator:\n",
    "            x_batch, y_labels = x_batch.to(device), y_labels.to(device)\n",
    "            y_out = model(x_batch, device)\n",
    "            ## predict label 1 or 0\n",
    "            y_pred = torch.round(y_out)\n",
    "            # print(y_pred)\n",
    "            accy_count = (y_pred==y_labels).sum().item()\n",
    "            accy_count_total += accy_count\n",
    "            total_count += len(y_labels)\n",
    "        ## overall accuracy\n",
    "        accy = accy_count_total/total_count*100\n",
    "        print(\"test accuracy: \", accy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "299fde79",
   "metadata": {
    "id": "299fde79",
    "outputId": "4f86b6f7-a7c1-4008-8ddc-358325f5d8b1",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****************************************************************************************\n",
      "start model training now\n",
      "*****************************************************************************************\n",
      "step:50 loss: 0.6879158818721771\n",
      "step:100 loss: 0.5474800729751587\n",
      "step:150 loss: 0.4414543652534485\n",
      "step:200 loss: 0.42285697281360624\n",
      "step:250 loss: 0.39608989030122754\n",
      "step:300 loss: 0.3731684562563896\n",
      "step:350 loss: 0.3449834924936295\n",
      "step:400 loss: 0.30728959262371064\n",
      "step:450 loss: 0.3116663745045662\n",
      "step:500 loss: 0.28051521360874176\n",
      "step:550 loss: 0.2926803186535835\n",
      "step:600 loss: 0.30002873033285143\n",
      "step:650 loss: 0.3008412256836891\n",
      "step:700 loss: 0.2148407918214798\n",
      "step:750 loss: 0.2065829226374626\n",
      "step:800 loss: 0.23087890431284905\n",
      "step:850 loss: 0.23433969259262086\n",
      "step:900 loss: 0.23183392137289047\n",
      "step:950 loss: 0.22074613481760025\n",
      "step:1000 loss: 0.18499259158968925\n",
      "step:1050 loss: 0.13070342965424062\n",
      "step:1100 loss: 0.13927300065755843\n",
      "step:1150 loss: 0.14772445395588873\n",
      "step:1200 loss: 0.16545291364192963\n",
      "step:1250 loss: 0.15823871329426764\n",
      "step:1300 loss: 0.16295522041618823\n",
      "step:1350 loss: 0.1063281636685133\n",
      "step:1400 loss: 0.08617197893559933\n",
      "step:1450 loss: 0.09816544204950332\n",
      "step:1500 loss: 0.09890890590846539\n",
      "step:1550 loss: 0.10440759256482124\n",
      "step:1600 loss: 0.1327708500623703\n",
      "step:1650 loss: 0.11945859901607037\n",
      "step:1700 loss: 0.07306292606517673\n",
      "step:1750 loss: 0.05242788450792432\n",
      "step:1800 loss: 0.06516340892761946\n",
      "step:1850 loss: 0.07496819701045751\n",
      "step:1900 loss: 0.07108169425278903\n",
      "step:1950 loss: 0.0759015916287899\n",
      "step:2000 loss: 0.04529713772237301\n",
      "step:2050 loss: 0.035482629779726264\n",
      "step:2100 loss: 0.04632431389763951\n",
      "step:2150 loss: 0.04947427585721016\n",
      "step:2200 loss: 0.046753989569842815\n",
      "step:2250 loss: 0.04969163574278355\n",
      "step:2300 loss: 0.0588407960254699\n",
      "step:2350 loss: 0.022017008322291077\n",
      "step:2400 loss: 0.0273160985019058\n",
      "step:2450 loss: 0.03188274901360273\n",
      "step:2500 loss: 0.03283263740129769\n",
      "step:2550 loss: 0.03258095140568912\n",
      "step:2600 loss: 0.028236593250185252\n",
      "step:2650 loss: 0.022451178054325283\n",
      "step:2700 loss: 0.02614735536510125\n",
      "step:2750 loss: 0.015533735086210072\n",
      "step:2800 loss: 0.018803833071142435\n",
      "step:2850 loss: 0.02213493613526225\n",
      "step:2900 loss: 0.031027694614604116\n",
      "step:2950 loss: 0.04548706834670156\n",
      "step:3000 loss: 0.018497206275351345\n",
      "step:3050 loss: 0.02145752115175128\n",
      "step:3100 loss: 0.02057796839158982\n",
      "step:3150 loss: 0.01881623033201322\n",
      "step:3200 loss: 0.013934458957519382\n",
      "step:3250 loss: 0.026766697987914086\n",
      "step:3300 loss: 0.024947004077257588\n",
      "step:3350 loss: 0.01231172633357346\n",
      "step:3400 loss: 0.017522723653819412\n",
      "step:3450 loss: 0.012231254525249823\n",
      "step:3500 loss: 0.016541626784019173\n",
      "step:3550 loss: 0.020297272705938667\n",
      "step:3600 loss: 0.01659506788942963\n",
      "step:3650 loss: 0.009206290740403347\n",
      "step:3700 loss: 0.01041901358170435\n",
      "step:3750 loss: 0.0057059834606479854\n",
      "step:3800 loss: 0.015313045227667316\n",
      "step:3850 loss: 0.011349992146715521\n",
      "step:3900 loss: 0.00897092655999586\n",
      "step:3950 loss: 0.010398960465099662\n",
      "step:4000 loss: 0.00783307227306068\n",
      "step:4050 loss: 0.003861962255323306\n",
      "step:4100 loss: 0.044295630006818104\n",
      "step:4150 loss: 0.01907164030126296\n",
      "step:4200 loss: 0.020362420247402042\n",
      "step:4250 loss: 0.01143568156985566\n",
      "step:4300 loss: 0.004905365605372936\n",
      "step:4350 loss: 0.005652225315570831\n",
      "step:4400 loss: 0.007821180053288117\n",
      "step:4450 loss: 0.008041446851857472\n",
      "step:4500 loss: 0.009927230044268072\n",
      "step:4550 loss: 0.0162020980194211\n",
      "step:4600 loss: 0.012558539914898574\n",
      "step:4650 loss: 0.003710552820703015\n",
      "step:4700 loss: 0.0065985364184598435\n",
      "step:4750 loss: 0.005438461756275501\n",
      "step:4800 loss: 0.012551879918319173\n",
      "step:4850 loss: 0.006808277804520912\n",
      "step:4900 loss: 0.006141290080267936\n",
      "step:4950 loss: 0.0028476680524181573\n",
      "step:5000 loss: 0.0023833355557871983\n",
      "step:5050 loss: 0.0033873936015879737\n",
      "step:5100 loss: 0.003692178884521127\n",
      "step:5150 loss: 0.002751304381235968\n",
      "step:5200 loss: 0.0018828966008732095\n",
      "step:5250 loss: 0.0065041224751621485\n",
      "step:5300 loss: 0.0027408518517768243\n",
      "step:5350 loss: 0.0015304605508572422\n",
      "step:5400 loss: 0.002660226491861977\n",
      "step:5450 loss: 0.0016434425700572318\n",
      "step:5500 loss: 0.0021647020539967343\n",
      "step:5550 loss: 0.005018007332109846\n",
      "step:5600 loss: 0.005765382295940071\n",
      "step:5650 loss: 0.013897585746162804\n",
      "step:5700 loss: 0.010041587239829824\n",
      "step:5750 loss: 0.00826787779427832\n",
      "step:5800 loss: 0.014347292935126462\n",
      "step:5850 loss: 0.022453426697757094\n",
      "step:5900 loss: 0.009905068392981775\n",
      "step:5950 loss: 0.004094298649579287\n",
      "step:6000 loss: 0.010442119029030437\n",
      "step:6050 loss: 0.006766054629115388\n",
      "step:6100 loss: 0.010667135664261878\n",
      "step:6150 loss: 0.01316836727259215\n",
      "step:6200 loss: 0.015500900930783245\n",
      "step:6250 loss: 0.02227268510381691\n",
      "step:6300 loss: 0.0058125504625786565\n",
      "step:6350 loss: 0.007137075375649147\n",
      "step:6400 loss: 0.009405807090806774\n",
      "step:6450 loss: 0.014021798093381221\n",
      "step:6500 loss: 0.006644948968169046\n",
      "step:6550 loss: 0.01649895105685573\n",
      "step:6600 loss: 0.005165483912569471\n",
      "step:6650 loss: 0.002581026913539972\n",
      "step:6700 loss: 0.006423706077912357\n",
      "step:6750 loss: 0.01025917356251739\n",
      "step:6800 loss: 0.005432516785804182\n",
      "step:6850 loss: 0.00833489877753891\n",
      "step:6900 loss: 0.006818628912151325\n",
      "step:6950 loss: 0.003554546694504097\n",
      "step:7000 loss: 0.006456501584762009\n",
      "step:7050 loss: 0.004788251129793935\n",
      "step:7100 loss: 0.004358346837689169\n",
      "step:7150 loss: 0.006527932531025726\n",
      "step:7200 loss: 0.010891639874025714\n",
      "step:7250 loss: 0.0017677253084548282\n",
      "step:7300 loss: 0.006179922619994613\n",
      "step:7350 loss: 0.006426800578337861\n",
      "step:7400 loss: 0.006144280600638013\n",
      "step:7450 loss: 0.006055940833030036\n",
      "step:7500 loss: 0.004085534354089759\n",
      "step:7550 loss: 0.0033179172043310244\n",
      "step:7600 loss: 0.001630385169191868\n",
      "step:7650 loss: 0.0015443191879603547\n",
      "step:7700 loss: 0.002163163398654433\n",
      "step:7750 loss: 0.000826469818566693\n",
      "step:7800 loss: 0.0037929781746061054\n",
      "step:7850 loss: 0.00662067794950417\n",
      "step:7900 loss: 0.001049375864677131\n",
      "step:7950 loss: 0.001634828564565396\n",
      "step:8000 loss: 0.0015373149998049484\n",
      "step:8050 loss: 0.00742659658924822\n",
      "step:8100 loss: 0.006389467035041889\n",
      "step:8150 loss: 0.0014817894742736827\n",
      "step:8200 loss: 0.0024419194294750925\n",
      "step:8250 loss: 0.0010457426263747037\n",
      "step:8300 loss: 0.0018293110111335408\n",
      "step:8350 loss: 0.0018720894575380953\n",
      "step:8400 loss: 0.00207685295608826\n",
      "step:8450 loss: 0.0040583269178023325\n",
      "step:8500 loss: 0.008435382591706002\n",
      "step:8550 loss: 0.0015768887395461206\n",
      "step:8600 loss: 0.002526116018780158\n",
      "step:8650 loss: 0.0027516364597249777\n",
      "step:8700 loss: 0.00593274131329963\n",
      "step:8750 loss: 0.004809212784020928\n",
      "step:8800 loss: 0.008321672889433102\n",
      "step:8850 loss: 0.013135464382976351\n",
      "step:8900 loss: 0.00207120163977379\n",
      "step:8950 loss: 0.0014079417410539463\n",
      "step:9000 loss: 0.0060853944900736675\n",
      "step:9050 loss: 0.006462251642660704\n",
      "step:9100 loss: 0.0028756575111765414\n",
      "step:9150 loss: 0.0012454648468155937\n",
      "step:9200 loss: 0.0020546287447359645\n",
      "step:9250 loss: 0.0022394178117565388\n",
      "step:9300 loss: 0.0028422156112355877\n",
      "step:9350 loss: 0.0009910531295463443\n",
      "step:9400 loss: 0.0009983550980541623\n",
      "step:9450 loss: 0.003694169503214653\n",
      "step:9500 loss: 0.0011007357107155257\n",
      "step:9550 loss: 0.0011079292784052085\n",
      "step:9600 loss: 0.005287650552527339\n",
      "step:9650 loss: 0.004728700254636351\n",
      "step:9700 loss: 0.0026512499102318544\n",
      "step:9750 loss: 0.00744190361830988\n",
      "step:9800 loss: 0.0026976831014326307\n",
      "step:9850 loss: 0.0022698692742415005\n",
      "step:9900 loss: 0.0012440845296077896\n",
      "step:9950 loss: 0.0007108233623330306\n",
      "step:10000 loss: 0.013137407683825587\n",
      "step:10050 loss: 0.0022827488844632173\n",
      "step:10100 loss: 0.0031408813883535914\n",
      "step:10150 loss: 0.0014142548947347678\n",
      "step:10200 loss: 0.0008116413750030915\n",
      "step:10250 loss: 0.0020933831734146224\n",
      "step:10300 loss: 0.002470247680321336\n",
      "step:10350 loss: 0.003480240884855448\n",
      "step:10400 loss: 0.0011728665789632942\n",
      "step:10450 loss: 0.004102289460570318\n",
      "step:10500 loss: 0.004848059878859203\n",
      "step:10550 loss: 0.008147283221533144\n",
      "step:10600 loss: 0.007659338909252256\n",
      "step:10650 loss: 0.013304046711346018\n",
      "step:10700 loss: 0.013199707544335979\n",
      "step:10750 loss: 0.0073611267789965495\n",
      "step:10800 loss: 0.002570211189231486\n",
      "step:10850 loss: 0.005241495581485651\n",
      "step:10900 loss: 0.005169156234478578\n",
      "step:10950 loss: 0.004796410485432716\n",
      "step:11000 loss: 0.003113281383702997\n",
      "step:11050 loss: 0.0012426846659946023\n",
      "step:11100 loss: 0.0024229651786117756\n",
      "step:11150 loss: 0.0005646146219532965\n",
      "step:11200 loss: 0.0006597294979746949\n",
      "step:11250 loss: 0.0042196802160469815\n",
      "step:11300 loss: 0.0037315233838126007\n",
      "step:11350 loss: 0.0012262785424809407\n",
      "step:11400 loss: 0.004906887515244307\n",
      "step:11450 loss: 0.0023191049922422734\n",
      "step:11500 loss: 0.002121941191762744\n",
      "step:11550 loss: 0.0024902289097371976\n",
      "step:11600 loss: 0.0013779978863021824\n",
      "step:11650 loss: 0.0008207521652002469\n",
      "step:11700 loss: 0.001590518718403473\n",
      "step:11750 loss: 0.0011734343770876876\n",
      "step:11800 loss: 0.0004753314994468383\n",
      "step:11850 loss: 0.00045449687726431873\n",
      "step:11900 loss: 0.00018165473830777045\n",
      "step:11950 loss: 0.0006924022388739104\n",
      "step:12000 loss: 0.0005188658328825113\n",
      "step:12050 loss: 0.004481938347835239\n",
      "step:12100 loss: 0.004839361544291023\n",
      "step:12150 loss: 0.0006165163853984268\n",
      "step:12200 loss: 0.001288697736999893\n",
      "step:12250 loss: 0.0008334992052186863\n",
      "step:12300 loss: 0.0014043253425006696\n",
      "step:12350 loss: 0.005851388614973985\n",
      "step:12400 loss: 0.002875121582765132\n",
      "step:12450 loss: 0.002425454415388231\n",
      "step:12500 loss: 0.000333761372949084\n",
      "step:12550 loss: 0.0002918578005301242\n",
      "step:12600 loss: 0.0014494743320483394\n",
      "step:12650 loss: 0.0016510348650081142\n",
      "step:12700 loss: 0.003117381139563804\n",
      "step:12750 loss: 0.008713915372281918\n",
      "step:12800 loss: 0.00192484172355762\n",
      "step:12850 loss: 0.0015646846232266398\n",
      "step:12900 loss: 0.000848510990235809\n",
      "step:12950 loss: 0.00034257109620739357\n",
      "step:13000 loss: 0.00046528008339919325\n",
      "step:13050 loss: 0.0005319346690521343\n",
      "step:13100 loss: 0.0008441309384897977\n",
      "step:13150 loss: 0.0029125409564949222\n",
      "step:13200 loss: 0.003956823191192598\n",
      "step:13250 loss: 0.010030195949875633\n",
      "step:13300 loss: 0.0019443743687224924\n",
      "step:13350 loss: 0.0027775994249441284\n",
      "step:13400 loss: 0.0016182907627080567\n",
      "step:13450 loss: 0.0017656827919381613\n",
      "step:13500 loss: 0.00040553585666657454\n",
      "step:13550 loss: 0.0010818261803797213\n",
      "step:13600 loss: 0.001869584805026534\n",
      "step:13650 loss: 0.001289012293673295\n",
      "step:13700 loss: 0.0006756415585005015\n",
      "step:13750 loss: 0.006561364113858872\n",
      "step:13800 loss: 0.0017480298204827704\n",
      "step:13850 loss: 0.0003426270679619847\n",
      "step:13900 loss: 0.0005145220647318638\n",
      "step:13950 loss: 0.00038783090992183133\n",
      "step:14000 loss: 0.0013718811301623645\n",
      "step:14050 loss: 0.00456479918218065\n",
      "step:14100 loss: 0.0020624152678101384\n",
      "step:14150 loss: 0.0028953734403603447\n",
      "step:14200 loss: 0.006140473545419809\n",
      "step:14250 loss: 0.0016395754490258695\n",
      "step:14300 loss: 0.004058972745360734\n",
      "step:14350 loss: 0.0014307713959715329\n",
      "step:14400 loss: 0.0028589289459887367\n",
      "step:14450 loss: 0.0008176276632639202\n",
      "step:14500 loss: 0.0018395959398321794\n",
      "step:14550 loss: 0.0021093573158759682\n",
      "step:14600 loss: 0.0007703604170501421\n",
      "step:14650 loss: 0.001993388264963869\n",
      "step:14700 loss: 0.0016430440626754716\n",
      "step:14750 loss: 0.0013503393770497496\n",
      "step:14800 loss: 0.0009476704048029205\n",
      "step:14850 loss: 0.00029495173903796966\n",
      "step:14900 loss: 0.0005148981434012967\n",
      "step:14950 loss: 0.0005971906839658913\n",
      "step:15000 loss: 0.00027769304309003927\n",
      "step:15050 loss: 0.0014868215010119456\n",
      "step:15100 loss: 0.0002892300036273809\n",
      "step:15150 loss: 0.00030490753001231495\n",
      "step:15200 loss: 0.0002988070424362377\n",
      "step:15250 loss: 0.0006333613185506693\n",
      "step:15300 loss: 0.00010696091823319875\n",
      "step:15350 loss: 0.00015749090124245413\n",
      "step:15400 loss: 9.697231447375999e-05\n",
      "step:15450 loss: 8.428476854760447e-05\n",
      "step:15500 loss: 5.7383974926779046e-05\n",
      "step:15550 loss: 0.00011949468622105996\n",
      "step:15600 loss: 0.0004556910106566647\n",
      "step:15650 loss: 0.00011239238975235821\n",
      "step:15700 loss: 0.00013990295849907852\n",
      "step:15750 loss: 4.725429767859168e-05\n",
      "step:15800 loss: 3.373905721787196e-05\n",
      "step:15850 loss: 4.747413850509474e-05\n",
      "step:15900 loss: 4.92798627033153e-05\n",
      "step:15950 loss: 4.025343465855258e-05\n",
      "step:16000 loss: 7.043247685942333e-05\n",
      "step:16050 loss: 2.470170512083314e-05\n",
      "step:16100 loss: 4.759088819355384e-05\n",
      "step:16150 loss: 2.286417682626052e-05\n",
      "step:16200 loss: 8.471859374481027e-05\n",
      "step:16250 loss: 0.0001090090233481078\n",
      "step:16300 loss: 3.9671950739830207e-05\n",
      "step:16350 loss: 2.400157213287457e-05\n",
      "step:16400 loss: 2.8381181268741785e-05\n",
      "step:16450 loss: 6.955418392351476e-05\n",
      "step:16500 loss: 2.2730835103175194e-05\n",
      "step:16550 loss: 3.865443952292935e-05\n",
      "step:16600 loss: 7.594256941786171e-05\n",
      "step:16650 loss: 0.0016493841182011692\n",
      "step:16700 loss: 0.0010718232511453608\n",
      "step:16750 loss: 0.005777480656506668\n",
      "step:16800 loss: 0.0009612678511803096\n",
      "step:16850 loss: 0.0007303697706583989\n",
      "step:16900 loss: 0.005302135099327643\n",
      "step:16950 loss: 0.014107058999215952\n",
      "step:17000 loss: 0.019356632223934866\n",
      "step:17050 loss: 0.007992145056296068\n",
      "step:17100 loss: 0.005040678017467144\n",
      "step:17150 loss: 0.003306701433521084\n",
      "step:17200 loss: 0.005364944269349508\n",
      "step:17250 loss: 0.0030075824561572517\n",
      "step:17300 loss: 0.002543959686772723\n",
      "step:17350 loss: 0.00570213915383647\n",
      "step:17400 loss: 0.00043385579887399216\n",
      "step:17450 loss: 0.003040440049726385\n",
      "step:17500 loss: 0.0027910992761644594\n",
      "step:17550 loss: 0.003295517773985921\n",
      "step:17600 loss: 0.0014769729988347536\n",
      "step:17650 loss: 0.0023302736680057023\n",
      "step:17700 loss: 0.000508187181339963\n",
      "step:17750 loss: 0.0026255598909301625\n",
      "step:17800 loss: 0.0008533427634210966\n",
      "step:17850 loss: 0.0005627405951690889\n",
      "step:17900 loss: 0.00032639811312492386\n",
      "step:17950 loss: 0.000609515781054597\n",
      "step:18000 loss: 0.0025870745809788787\n",
      "step:18050 loss: 0.0013454823929168924\n",
      "step:18100 loss: 0.0006745100192597419\n",
      "step:18150 loss: 0.00037452509098329757\n",
      "step:18200 loss: 0.00023556278992145963\n",
      "step:18250 loss: 0.00014330423950241312\n",
      "step:18300 loss: 0.00012709991292013002\n",
      "step:18350 loss: 0.0012277421709768533\n",
      "step:18400 loss: 0.00017961550494874245\n",
      "step:18450 loss: 6.285567580562201e-05\n",
      "step:18500 loss: 0.0005625458643658021\n",
      "step:18550 loss: 0.00011883229711202149\n",
      "step:18600 loss: 7.919324726117338e-05\n",
      "step:18650 loss: 8.403504288764907e-05\n",
      "step:18700 loss: 6.42438351974306e-05\n",
      "step:18750 loss: 4.031370925190458e-05\n",
      "step:18800 loss: 0.0001473766278104449\n",
      "step:18850 loss: 9.421125692142596e-05\n",
      "step:18900 loss: 7.247830876281114e-05\n",
      "step:18950 loss: 0.00042175946906922943\n",
      "step:19000 loss: 6.276217267895845e-05\n",
      "step:19050 loss: 4.559505206771064e-05\n",
      "step:19100 loss: 4.632774898709613e-05\n",
      "step:19150 loss: 6.396765634235635e-05\n",
      "step:19200 loss: 4.1814825195842786e-05\n",
      "step:19250 loss: 6.477780038721903e-05\n",
      "step:19300 loss: 3.4202079135638996e-05\n",
      "step:19350 loss: 2.3798705159947532e-05\n",
      "step:19400 loss: 2.1898335944570135e-05\n",
      "step:19450 loss: 2.513680977244803e-05\n",
      "step:19500 loss: 1.4608584134521152e-05\n",
      "step:19550 loss: 2.5023739912057863e-05\n",
      "step:19600 loss: 2.2356121230586724e-05\n",
      "step:19650 loss: 3.4009577702818206e-05\n",
      "step:19700 loss: 2.554271438782507e-05\n",
      "step:19750 loss: 2.4317187571796238e-05\n",
      "step:19800 loss: 0.002366778146665638\n",
      "step:19850 loss: 0.0011502034871728028\n",
      "step:19900 loss: 0.00025813331214067145\n",
      "step:19950 loss: 0.002384722352862809\n",
      "step:20000 loss: 0.0032378234086809245\n",
      "step:20050 loss: 0.005179461389652715\n",
      "step:20100 loss: 0.0019749303532080376\n",
      "step:20150 loss: 0.003520581235470672\n",
      "step:20200 loss: 0.00366777172219372\n",
      "step:20250 loss: 0.0059716983102043745\n",
      "step:20300 loss: 0.00496850922358135\n",
      "step:20350 loss: 0.0003468252527090954\n",
      "step:20400 loss: 0.0013315721595745344\n",
      "step:20450 loss: 0.000445257649589621\n",
      "step:20500 loss: 0.003991453440112309\n",
      "step:20550 loss: 0.011958947687612636\n",
      "step:20600 loss: 0.006585589530004654\n",
      "step:20650 loss: 0.0033048808135390574\n",
      "step:20700 loss: 0.0009857129511442507\n",
      "step:20750 loss: 0.0016004187096086754\n",
      "step:20800 loss: 0.0034540750449059488\n",
      "step:20850 loss: 0.0027679644926411127\n",
      "step:20900 loss: 0.005767513796449748\n",
      "step:20950 loss: 0.003521801705028338\n",
      "step:21000 loss: 0.0019461714605404268\n",
      "step:21050 loss: 0.0031066723407457177\n",
      "step:21100 loss: 0.0008772863953527122\n",
      "step:21150 loss: 0.0019054947111362707\n",
      "step:21200 loss: 0.0012303120462638618\n",
      "step:21250 loss: 0.0013640430229315825\n",
      "step:21300 loss: 0.004004764782830534\n",
      "step:21350 loss: 0.0006201480056688524\n",
      "step:21400 loss: 0.000220499817783093\n",
      "step:21450 loss: 0.0018381334021660223\n",
      "step:21500 loss: 0.0007105233614402096\n",
      "step:21550 loss: 0.00020000314526441798\n",
      "step:21600 loss: 0.00016908100615637523\n",
      "step:21650 loss: 7.498705544321638e-05\n",
      "step:21700 loss: 5.292241202596415e-05\n",
      "step:21750 loss: 9.991253013595269e-05\n",
      "step:21800 loss: 0.00022868875196536465\n",
      "step:21850 loss: 0.002220914956305933\n",
      "step:21900 loss: 0.0006136483891896205\n",
      "step:21950 loss: 9.996208906841275e-05\n",
      "step:22000 loss: 0.00010480830501137461\n",
      "step:22050 loss: 4.2747629995574245e-05\n",
      "step:22100 loss: 5.01058135682797e-05\n",
      "step:22150 loss: 6.424362930374628e-05\n",
      "step:22200 loss: 0.0005798163995723371\n",
      "step:22250 loss: 9.178741402138257e-05\n",
      "step:22300 loss: 5.375377228006073e-05\n",
      "step:22350 loss: 2.6960071822941243e-05\n",
      "step:22400 loss: 6.221353860041745e-05\n",
      "step:22450 loss: 5.004390166277517e-05\n",
      "step:22500 loss: 9.352684527243583e-05\n",
      "step:22550 loss: 3.5511170042923365e-05\n",
      "step:22600 loss: 5.111057903832261e-05\n",
      "step:22650 loss: 3.2484565682580066e-05\n",
      "step:22700 loss: 3.682574985305109e-05\n",
      "step:22750 loss: 2.8557397299664445e-05\n",
      "step:22800 loss: 1.4709345609844604e-05\n",
      "step:22850 loss: 1.8203300294317158e-05\n",
      "step:22900 loss: 4.33722099845113e-05\n",
      "step:22950 loss: 2.087288921700292e-05\n",
      "step:23000 loss: 0.00017092914264821957\n",
      "step:23050 loss: 3.596069157993042e-05\n",
      "step:23100 loss: 1.0762615895032468e-05\n",
      "step:23150 loss: 3.4133211961488996e-05\n",
      "step:23200 loss: 7.507927174287942e-05\n",
      "step:23250 loss: 1.6568734590691747e-05\n",
      "step:23300 loss: 6.040102777859602e-05\n",
      "step:23350 loss: 2.527425191033217e-05\n",
      "step:23400 loss: 1.3851614404529755e-05\n",
      "step:23450 loss: 2.5929199653091926e-05\n",
      "step:23500 loss: 9.31371459444108e-06\n",
      "step:23550 loss: 1.555621852958211e-05\n",
      "step:23600 loss: 0.00011665711464843298\n",
      "step:23650 loss: 2.4969141176711673e-05\n",
      "step:23700 loss: 1.992743710729883e-05\n",
      "step:23750 loss: 1.1903266943136259e-05\n",
      "step:23800 loss: 9.775017790047969e-06\n",
      "step:23850 loss: 1.0393645532644768e-05\n",
      "step:23900 loss: 3.407938911209385e-05\n",
      "step:23950 loss: 9.017624940952373e-06\n",
      "step:24000 loss: 1.0752261992763579e-05\n",
      "step:24050 loss: 1.7070154257226022e-05\n",
      "step:24100 loss: 1.1598866424264997e-05\n",
      "step:24150 loss: 7.772903808358933e-05\n",
      "step:24200 loss: 5.862993849632403e-06\n",
      "step:24250 loss: 1.7237188224044076e-05\n",
      "step:24300 loss: 9.61021420096131e-06\n",
      "step:24350 loss: 6.615546858768084e-06\n",
      "step:24400 loss: 5.479952735072402e-06\n",
      "step:24450 loss: 2.8024832920436894e-05\n",
      "step:24500 loss: 1.0162343951378716e-05\n",
      "step:24550 loss: 1.2197487631055993e-05\n",
      "step:24600 loss: 7.248344346351132e-06\n",
      "step:24650 loss: 6.23068897596113e-06\n",
      "step:24700 loss: 4.239703019578655e-05\n",
      "step:24750 loss: 1.9086409719193397e-05\n",
      "step:24800 loss: 1.5416346589063323e-05\n",
      "step:24850 loss: 1.0448673286305166e-05\n",
      "step:24900 loss: 4.858257851765302e-06\n",
      "step:24950 loss: 8.462137668061587e-06\n",
      "step:25000 loss: 4.171535091217038e-05\n",
      "step:25050 loss: 1.5342759134000517e-05\n",
      "step:25100 loss: 8.775374253531254e-06\n",
      "step:25150 loss: 7.98074237820856e-06\n",
      "step:25200 loss: 4.371692854761022e-06\n",
      "step:25250 loss: 1.3194067980748514e-05\n",
      "step:25300 loss: 2.2208557079750336e-05\n",
      "step:25350 loss: 0.00014076868389395258\n",
      "step:25400 loss: 7.301896716853662e-05\n",
      "step:25450 loss: 4.075140145687328e-06\n",
      "step:25500 loss: 1.1633595008788688e-05\n",
      "step:25550 loss: 0.0007323226377639003\n",
      "step:25600 loss: 0.006779498672971158\n",
      "step:25650 loss: 0.006883200866432162\n",
      "step:25700 loss: 0.009820901757375395\n",
      "step:25750 loss: 0.004608436546404846\n",
      "step:25800 loss: 0.005156504148326349\n",
      "step:25850 loss: 0.006844137430171031\n",
      "step:25900 loss: 0.005319639953886508\n",
      "step:25950 loss: 0.004710137918609689\n",
      "step:26000 loss: 0.004408990329811786\n",
      "step:26050 loss: 0.004961105571019289\n",
      "step:26100 loss: 0.005043383489664848\n",
      "step:26150 loss: 0.006923618793261994\n",
      "step:26200 loss: 0.004295023467884676\n",
      "step:26250 loss: 0.0005134374907811434\n",
      "step:26300 loss: 0.0010294845350563265\n",
      "step:26350 loss: 0.002660800808116619\n",
      "step:26400 loss: 0.005441588839330507\n",
      "step:26450 loss: 0.0027333043543876557\n",
      "step:26500 loss: 0.001033292518000053\n",
      "step:26550 loss: 0.00021321850599179015\n",
      "step:26600 loss: 0.0002555139329547273\n",
      "step:26650 loss: 0.0009808659760892624\n",
      "step:26700 loss: 0.00026606582655404055\n",
      "step:26750 loss: 0.0011134649675568653\n",
      "step:26800 loss: 0.00020868351980027455\n",
      "step:26850 loss: 6.555833240327047e-05\n",
      "step:26900 loss: 0.00017198699199980183\n",
      "step:26950 loss: 6.700121053540898e-05\n",
      "step:27000 loss: 0.000885277051149842\n",
      "step:27050 loss: 0.0029621620100215296\n",
      "step:27100 loss: 0.00016853459573212603\n",
      "step:27150 loss: 0.00013362566541587738\n",
      "step:27200 loss: 0.00018150259558751713\n",
      "step:27250 loss: 8.131418977882277e-05\n",
      "step:27300 loss: 4.8253594702600825e-05\n",
      "step:27350 loss: 0.0001379073632233485\n",
      "step:27400 loss: 3.403239656108781e-05\n",
      "step:27450 loss: 0.00011103364599591714\n",
      "step:27500 loss: 3.009162330613435e-05\n",
      "step:27550 loss: 1.7795525835708758e-05\n",
      "step:27600 loss: 0.0002544391720880412\n",
      "step:27650 loss: 7.696226777056836e-05\n",
      "step:27700 loss: 4.133456888780529e-05\n",
      "step:27750 loss: 2.099599319990375e-05\n",
      "step:27800 loss: 2.2997243374902608e-05\n",
      "step:27850 loss: 4.046603343567767e-05\n",
      "step:27900 loss: 6.733081757630544e-05\n",
      "step:27950 loss: 2.2589068971683447e-05\n",
      "step:28000 loss: 2.789812288142457e-05\n",
      "step:28050 loss: 2.3915585792622096e-05\n",
      "step:28100 loss: 1.3238144996705614e-05\n",
      "step:28150 loss: 1.4718912108264704e-05\n",
      "step:28200 loss: 8.504897690500002e-06\n",
      "step:28250 loss: 8.046384794113237e-05\n",
      "step:28300 loss: 2.91763680911572e-05\n",
      "step:28350 loss: 1.1825792436752635e-05\n",
      "step:28400 loss: 1.0345711900754396e-05\n",
      "step:28450 loss: 5.1963561760999253e-05\n",
      "step:28500 loss: 5.292483399443881e-06\n",
      "step:28550 loss: 1.121902487625448e-05\n",
      "step:28600 loss: 6.584624510423964e-06\n",
      "step:28650 loss: 5.537645631761734e-05\n",
      "step:28700 loss: 2.452682695547992e-05\n",
      "step:28750 loss: 3.738730582739436e-05\n",
      "step:28800 loss: 2.727512053922965e-05\n",
      "step:28850 loss: 1.596901248035465e-05\n",
      "step:28900 loss: 2.1065188665261303e-05\n",
      "step:28950 loss: 8.22217627273858e-06\n",
      "step:29000 loss: 7.053163786849837e-06\n",
      "step:29050 loss: 2.5671387628563025e-05\n",
      "step:29100 loss: 8.213629986357773e-06\n",
      "step:29150 loss: 9.539534199234367e-06\n",
      "step:29200 loss: 1.4152124654458475e-05\n",
      "step:29250 loss: 1.1738913989063348e-05\n",
      "step:29300 loss: 6.110381817734378e-06\n",
      "step:29350 loss: 8.311309701184654e-05\n",
      "step:29400 loss: 0.000350844571692619\n",
      "step:29450 loss: 0.0005267960520609449\n",
      "step:29500 loss: 0.00015198627546396891\n",
      "step:29550 loss: 0.0015171742196878313\n",
      "step:29600 loss: 0.002617118176665372\n",
      "step:29650 loss: 0.01270360763347071\n",
      "step:29700 loss: 0.0049676320724756804\n",
      "step:29750 loss: 0.005670343240890361\n",
      "step:29800 loss: 0.006353067039108282\n",
      "step:29850 loss: 0.001525382605523191\n",
      "step:29900 loss: 0.002060765778351197\n",
      "step:29950 loss: 0.0006694745904860611\n",
      "step:30000 loss: 0.0026335072467918506\n",
      "step:30050 loss: 0.0026331063455472758\n",
      "step:30100 loss: 0.0016173222834868284\n",
      "step:30150 loss: 0.00043666172988650944\n",
      "step:30200 loss: 0.00029778970124880286\n",
      "step:30250 loss: 0.00016556443572881107\n",
      "step:30300 loss: 0.00011188366460828546\n",
      "step:30350 loss: 0.00039602649386552005\n",
      "step:30400 loss: 0.00010252847735955583\n",
      "step:30450 loss: 6.616164790557377e-05\n",
      "step:30500 loss: 2.6334599285746664e-05\n",
      "step:30550 loss: 4.4694209473163935e-05\n",
      "step:30600 loss: 5.5394139415057e-05\n",
      "step:30650 loss: 3.8238788423541333e-05\n",
      "step:30700 loss: 3.3236416808222205e-05\n",
      "step:30750 loss: 3.792113710915146e-05\n",
      "step:30800 loss: 0.0003979623222116402\n",
      "step:30850 loss: 0.00040036076538058296\n",
      "step:30900 loss: 4.2278376076865244e-05\n",
      "step:30950 loss: 1.249265427759383e-05\n",
      "step:31000 loss: 0.0001319499443013683\n",
      "step:31050 loss: 8.462641857136077e-05\n",
      "step:31100 loss: 1.3844571815297969e-05\n",
      "step:31150 loss: 0.00011989176867615469\n",
      "step:31200 loss: 4.349929703607813e-05\n",
      "step:31250 loss: 4.274837933280651e-05\n",
      "step:31300 loss: 0.00013179549896790376\n",
      "step:31350 loss: 2.055071934165653e-05\n",
      "step:31400 loss: 2.9868540625557216e-05\n",
      "step:31450 loss: 1.1636800347787358e-05\n",
      "step:31500 loss: 6.902489518054722e-05\n",
      "step:31550 loss: 3.4120251720253235e-05\n",
      "step:31600 loss: 9.866242312455142e-06\n",
      "step:31650 loss: 1.4886224347279153e-05\n",
      "step:31700 loss: 1.3496071468352965e-05\n",
      "step:31750 loss: 1.4692533041511523e-05\n",
      "step:31800 loss: 1.0397088769735775e-05\n",
      "step:31850 loss: 2.4417044231199724e-05\n",
      "step:31900 loss: 1.1475367332707264e-05\n",
      "step:31950 loss: 4.896713078892389e-06\n",
      "step:32000 loss: 1.4058945120751787e-05\n",
      "step:32050 loss: 2.8466223505176912e-05\n",
      "step:32100 loss: 9.943813642792065e-06\n",
      "step:32150 loss: 6.42362298219723e-05\n",
      "step:32200 loss: 4.862532199638281e-05\n",
      "step:32250 loss: 4.69889053931638e-06\n",
      "step:32300 loss: 9.90596514839126e-06\n",
      "step:32350 loss: 9.91105773664458e-06\n",
      "step:32400 loss: 7.347897233955791e-06\n",
      "step:32450 loss: 3.6749797872914768e-06\n",
      "step:32500 loss: 1.1048429531399506e-05\n",
      "step:32550 loss: 3.692474156906656e-05\n",
      "step:32600 loss: 8.189544390688752e-06\n",
      "step:32650 loss: 1.0474437262502079e-05\n",
      "step:32700 loss: 3.943360290890041e-05\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----model testing now----\n",
      "test accuracy:  81.96\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84792a29",
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1718294832640,
     "user": {
      "displayName": "Nhat Le",
      "userId": "15128282853851571850"
     },
     "user_tz": 300
    },
    "id": "84792a29"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.-1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
